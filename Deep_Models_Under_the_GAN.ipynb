{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Models Under the GAN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PZw9Tnt2inh1",
        "_1n87zf8iHCG",
        "fInoWlV-id42",
        "gPaMtVwyjBxX"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhmmyF7ZfpMW"
      },
      "source": [
        "### Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning\n",
        "\n",
        "代码复现"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZw9Tnt2inh1"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDl4z2lAiq8t"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbOVPGPhTaF"
      },
      "source": [
        "## Parameter settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dmc9_6ceqs1"
      },
      "source": [
        "#Parameters for collaborative learning\n",
        "whether_warm_up = True\n",
        "warm_up_epochs = 7\n",
        "warm_up_batch_size = 256\n",
        "warm_up_learning_rate = 0.001\n",
        "warm_up_data_size = 600\n",
        "\n",
        "Train_round = 200\n",
        "Client_learning_rate = 0.0001\n",
        "Client_batch_size = 256\n",
        "\n",
        "Parameter_shape = []\n",
        "Parameter_number = 0\n",
        "Total_data_amount = 0\n",
        "\n",
        "Test_acc = 0 #updated after each round\n",
        "#Parameters for differential privicy\n",
        "Whether_differential_privacy = False\n",
        "Theta_d = 1\n",
        "Theta_u = 0.9\n",
        "Gamma = 0.003\n",
        "Tua = 0.0001\n",
        "Privacy_budget_per_para = 100\n",
        "\n",
        "\n",
        "#Parameters for GAN\n",
        "Whether_attack = True\n",
        "Fake_label = 10\n",
        "Attack_label = 3\n",
        "Start_attack_acc = 0.9\n",
        "Epoch_per_attack = 200\n",
        "Noise_dimension = 100\n",
        "Generated_picture_number = 256 #生成的图像数量，用于训练GAN\n",
        "Generator_optimizer = tf.keras.optimizers.SGD(learning_rate=0.0003)\n",
        "Discriminator_optimizer = tf.keras.optimizers.SGD(learning_rate=0.0003)\n",
        "\n",
        "Num_examples_to_generate = 256 #生成的图像数量，用于引诱其他clients输出更多信息\n",
        "\n",
        "Num_generate_for_show = 36 #生成的图像数量来展示\n",
        "\n",
        "#Parameters for test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdFVjeZ_hYyd"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjXbRzmFhYM9"
      },
      "source": [
        "# The discriminator model is the same as the collaboratively trained model\n",
        "def make_discriminator_model():\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Conv2D(32, (5, 5), strides=(1, 1), padding='same', input_shape=[28, 28, 1]))\n",
        "  model.add(keras.layers.Activation('tanh'))\n",
        "  model.add(keras.layers.MaxPool2D(pool_size=(3,3),strides=None, padding='valid'))\n",
        "\n",
        "  model.add(keras.layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same'))\n",
        "  model.add(keras.layers.Activation('tanh'))\n",
        "  model.add(keras.layers.MaxPool2D(pool_size=(2,2),strides=None, padding='valid'))\n",
        "\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(256,activation='tanh'))\n",
        "  model.add(keras.layers.Dense(200,activation='tanh'))\n",
        "  model.add(keras.layers.Dense(11))#activation=tf.nn.log_softmax))\n",
        "  return model\n",
        "\n",
        "# Malicious generator model\n",
        "def make_generator_model():\n",
        "  model = keras.Sequential()\n",
        "    \n",
        "  model.add(keras.layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  model.add(keras.layers.ReLU())\n",
        "\n",
        "  model.add(keras.layers.Reshape((7, 7, 256)))\n",
        "  assert model.output_shape == (None, 7, 7, 256)  # Batch size is not limited\n",
        "\n",
        "  model.add(keras.layers.Conv2DTranspose(128, (4, 4), strides=(1, 1), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 7, 7, 128)\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  model.add(keras.layers.ReLU())\n",
        "\n",
        "  model.add(keras.layers.Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 14, 14, 64)\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "  model.add(keras.layers.ReLU())\n",
        "\n",
        "  model.add(keras.layers.Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1n87zf8iHCG"
      },
      "source": [
        "## Differential Privacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q47ccxX3ic2G"
      },
      "source": [
        "def flatten(W_or_G):\n",
        "  flattened = np.array([])\n",
        "  for i in range(len(W_or_G)):\n",
        "    flattened = np.concatenate((flattened, W_or_G[i].flatten()))\n",
        "\n",
        "  return flattened\n",
        "def reshape(Flattened,para_shapes): \n",
        "#Flattened: a np.array of all parameters.\n",
        "#para_shapes: a list of tulpes representing the number and shape of parameters of neural networks layers\n",
        "  reshaped_Parameter = []\n",
        "  for p_num,shape in para_shapes:\n",
        "    reshaped_Parameter.append(np.reshape(Flattened[0:p_num],shape))\n",
        "    Flattened = Flattened[p_num:]\n",
        "  return reshaped_Parameter\n",
        "  \n",
        "def sigma(x,c,delta_f):\n",
        "  return 2*c*delta_f/x\n",
        "def differential_privacy(Gradient,privacy_budget_per_para,gamma,theta,tua,parameter_shape):\n",
        "  #time1 = time.time()\n",
        "  Gradient_flattened = flatten(Gradient)\n",
        "  parameter_number = len(Gradient_flattened)\n",
        "  c = floor(theta*parameter_number)\n",
        "  epsilon = privacy_budget_per_para*c\n",
        "  #print(epsilon,c,parameter_number,theta)\n",
        "  epsilon_1 = 8/9*epsilon\n",
        "  epsilon_2 = 2/9*epsilon\n",
        "  sigma_1 = sigma(epsilon_1,c,2*gamma)\n",
        "  sigma_2 = sigma(epsilon_2,c,2*gamma)\n",
        "  Tua_with_noise = np.random.laplace(0,sigma_1,parameter_number)+tua\n",
        "  R_w = np.random.laplace(0,2*sigma_1,parameter_number)\n",
        "  Gradient_flattened[Gradient_flattened>gamma] = gamma\n",
        "  Gradient_flattened[Gradient_flattened<-gamma] = -gamma\n",
        "  Gradient_with_noise_flat1 = np.absolute(Gradient_flattened)+R_w\n",
        "  index = []\n",
        "  for i in range(parameter_number):\n",
        "    if(Gradient_with_noise_flat1[i]>=Tua_with_noise[i]):\n",
        "      index.append(i)\n",
        "  if(len(index)>=c):\n",
        "    np.random.shuffle(index)\n",
        "    index = index[0:c]\n",
        "  R_w2 = np.random.laplace(0,sigma_2,parameter_number)\n",
        "  Gradient_with_noise_flat2 = R_w2+Gradient_flattened\n",
        "  tmp = np.zeros(parameter_number)\n",
        "  for i in index:\n",
        "    tmp[i] = Gradient_with_noise_flat2[i]\n",
        "  tmp = reshape(tmp,parameter_shape)\n",
        "  #print(\"total time for DP:\",time.time()-time1)\n",
        "  return tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fInoWlV-id42"
      },
      "source": [
        "## Client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrC7EDnjjAre"
      },
      "source": [
        "cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "def generator_loss(fake_output,attack_label):\n",
        "  ideal_result = np.zeros(len(fake_output))+attack_label\n",
        "  # for i in range(len(ideal_result)):\n",
        "  #   # The class which attacker intends to get\n",
        "  #   ideal_result[i] = attack_label\n",
        "  return cross_entropy(ideal_result,fake_output)\n",
        "\n",
        "#Discriminator needs to recognize the generated images\n",
        "def discriminator_loss(fake_output):    \n",
        "  fake_result = np.zeros(len(fake_output))+Fake_label\n",
        "  return cross_entropy(fake_result, fake_output)\n",
        "\n",
        "#Clients's behavior\n",
        "class Client:\n",
        "  def __init__(self,id):\n",
        "    self.model = make_discriminator_model()\n",
        "    self.model.compile(optimizer=keras.optimizers.Adam(learning_rate=Client_learning_rate),\n",
        "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                       metrics=['accuracy'])\n",
        "    server_Weight = server.send(1)\n",
        "    self.model.set_weights(server_Weight)\n",
        "    self.data = None\n",
        "    self.labels = None\n",
        "    self.data_amount = 0\n",
        "    self.id = id\n",
        "\n",
        "    self.malicious = False\n",
        "    self.attack_label = None\n",
        "    self.generator = None\n",
        "    self.attack_round = 0\n",
        "  def training_step(self):\n",
        "    # global server\n",
        "    Weight1 = self.model.get_weights()\n",
        "    Weight2 = server.send(Theta_d)\n",
        "    self.model.set_weights(Weight2)\n",
        "    if(self.malicious==True and Test_acc>Start_attack_acc and self.model.evaluate(test_images,test_labels,verbose=0)[1] > Start_attack_acc):\n",
        "      self.attack()\n",
        "    \n",
        "    self.model.fit(self.data,self.labels,validation_split=0,epochs=1,batch_size=Client_batch_size,verbose=1)\n",
        "    Gradient = np.array(self.model.get_weights()) - np.array(Weight1)\n",
        "    #start = time.time()\n",
        "    if Whether_differential_privacy == True:\n",
        "      return self.data_amount,differential_privacy(Gradient,Privacy_budget_per_para,Gamma,Theta_u,Tua,Parameter_shape)\n",
        "\n",
        "    #print(\"Time spend on DP:\",time.time()-start)\n",
        "    return self.data_amount,Gradient\n",
        "  def attack(self):\n",
        "    print(\"Attack round \",self.attack_round,\"begins.\")\n",
        "    self.attack_round = self.attack_round + 1\n",
        "    Gen_loss = []\n",
        "    Disc_loss = []\n",
        "    for i in range(Epoch_per_attack):\n",
        "      if i%100==0 :\n",
        "        print(i)\n",
        "      noise = tf.random.normal([Generated_picture_number,Noise_dimension])\n",
        "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = self.generator(noise, training=True)\n",
        "        fake_output = self.model(generated_images, training=False)\n",
        "        gen_loss = generator_loss(fake_output,self.attack_label)\n",
        "        Gen_loss.append(gen_loss)\n",
        "        disc_loss = discriminator_loss(fake_output)\n",
        "        Disc_loss.append(disc_loss)\n",
        "      Gradient_of_generator = gen_tape.gradient(gen_loss,self.generator.trainable_variables)\n",
        "      Generator_optimizer.apply_gradients(zip(Gradient_of_generator,self.generator.trainable_variables))\n",
        "      Gradients_of_discriminator = disc_tape.gradient(disc_loss, self.model.trainable_variables)\n",
        "      Discriminator_optimizer.apply_gradients(zip(Gradients_of_discriminator, self.model.trainable_variables))\n",
        "    plt.plot(Gen_loss,label='Gen_loss')\n",
        "    plt.plot(Disc_loss,label = 'Disc_loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    noise = tf.random.normal([Num_examples_to_generate, Noise_dimension])\n",
        "    \n",
        "    generated_Image = self.generator(noise,training = False)\n",
        "    malicious_Image = np.array(generated_Image)\n",
        "    malicious_Label = np.array([self.labels[10]]*Num_examples_to_generate)\n",
        "    self.data = self.data[0:self.data_amount]\n",
        "    self.labels = self.labels[0:self.data_amount]\n",
        "    self.data=np.concatenate((self.data,malicious_Image),axis = 0)\n",
        "    self.labels=np.concatenate((self.labels,malicious_Label),axis = 0)\n",
        "    self.generate_image(0)\n",
        "  def generate_image(self,round):\n",
        "    if self.malicious == False:\n",
        "      assert 0,\"I am innocent and would not generate any images\"\n",
        "    noise = tf.random.normal([Num_generate_for_show, Noise_dimension])\n",
        "    generated_Image = self.generator(noise,training = False)\n",
        "    fig = plt.figure(figsize=(6,6))\n",
        "    for i in range(Num_generate_for_show):\n",
        "      plt.subplot(6, 6, i+1)\n",
        "      plt.imshow(generated_Image[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPaMtVwyjBxX"
      },
      "source": [
        "## Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_19JhQd0jDgm"
      },
      "source": [
        "# Server's behavior\n",
        "class Server:\n",
        "  def __init__(self):\n",
        "    self.model = make_discriminator_model()\n",
        "    self.model.compile(optimizer=keras.optimizers.Adam(learning_rate=warm_up_learning_rate),\n",
        "                       loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                       metrics=['accuracy'])\n",
        "    self.warm_up_data = None\n",
        "    self.warm_up_labels = None\n",
        "    self.warm_up_data_amount = 0\n",
        "    self.stat = []\n",
        "    for i in range(Parameter_number):\n",
        "      self.stat.append([i,random.randint(0,5)])\n",
        "\n",
        "  def warm_up(self):\n",
        "    assert self.warm_up_data_amount != 0,\"There is no data for warm up!\"\n",
        "    self.model.fit(self.warm_up_data,self.warm_up_labels,validation_split=0,epochs=warm_up_epochs,batch_size=warm_up_batch_size,verbose=1)\n",
        "\n",
        "  #A participant uploads gradients\n",
        "  def new_gradient(self,Gradient):\n",
        "    Weight = self.model.get_weights()\n",
        "    Weight2 = Weight + Gradient\n",
        "    # Gradient_flattened = flatten(Gradient)\n",
        "    # for i in range(len(Gradient_flattened)):\n",
        "    #   if(Gradient_flattened[i] != 0):\n",
        "    #     self.stat[i][1] = self.stat[i][1]+1\n",
        "    self.model.set_weights(Weight2)\n",
        "    \n",
        "  #A participant downloads parameters.\n",
        "  def send(self,theta):\n",
        "    # largest_stat_index = [i[0] for i in sorted(self.stat,key = lambda x:(x[1],x[0]),reverse=True)[0:math.floor(theta*Parameter_number)]]\n",
        "    # Weight=self.model.get_weights()\n",
        "    # Weight_flattened = flatten(Weight)\n",
        "\n",
        "    # selected_Weight = np.zeros(Parameter_number)\n",
        "    # for i in largest_stat_index:\n",
        "    #   selected_Weight[i] = Weight_flattened[i]\n",
        "    # return selected_Weight,largest_stat_index\n",
        "    #do not test the theta_d\n",
        "    return self.model.get_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWSOfmZAjJ8d"
      },
      "source": [
        "## Collaboratively learning preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5gBnlG-jTfV",
        "outputId": "d66cd332-ca60-4539-adfd-a2b1e75446c0"
      },
      "source": [
        "#get Parameter number and shape\n",
        "test_model = make_discriminator_model()\n",
        "test_weight = test_model.get_weights()\n",
        "for w in test_weight:\n",
        "  shape = w.shape\n",
        "  p_num = 1\n",
        "  for i in shape:\n",
        "    p_num = p_num*i\n",
        "  Parameter_shape.append((p_num,w.shape))\n",
        "  Parameter_number = Parameter_number + p_num \n",
        "del test_model,test_weight\n",
        "\n",
        "#load data\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5   # Normalization\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
        "test_images = (test_images - 127.5) / 127.5   # Normalization\n",
        "Total_data_amount = len(train_labels)\n",
        "###shuffle the data to get the warm up data\n",
        "state = np.random.get_state()\n",
        "np.random.shuffle(train_images)\n",
        "np.random.set_state(state)\n",
        "np.random.shuffle(train_labels)\n",
        "warm_up_data = train_images[0:warm_up_data_size]\n",
        "warm_up_labels = train_labels[0:warm_up_data_size]\n",
        "\n",
        "#init server and clients\n",
        "server = Server()\n",
        "server.warm_up_data = warm_up_data\n",
        "server.warm_up_labels = warm_up_labels\n",
        "server.warm_up_data_amount = warm_up_data_size\n",
        "if whether_warm_up == True:\n",
        "  server.warm_up()\n",
        "clients = []\n",
        "for i in range(10):\n",
        "  client = Client(i)\n",
        "  client.data = train_images[train_labels==i]\n",
        "  client.labels = train_labels[train_labels==i]\n",
        "  client.data_amount = len(client.data)\n",
        "\n",
        "  state = np.random.get_state()\n",
        "  np.random.shuffle(client.data)\n",
        "  np.random.set_state(state)\n",
        "  np.random.shuffle(client.labels)\n",
        "  clients.append(client)\n",
        "del train_images,train_labels\n",
        "\n",
        "if Whether_attack == True:\n",
        "  clients[0].malicious = True\n",
        "  clients[0].attack_label = Attack_label\n",
        "  clients[0].generator = make_generator_model()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/7\n",
            "3/3 [==============================] - 31s 55ms/step - loss: 2.3008 - accuracy: 0.1700\n",
            "Epoch 2/7\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.6713 - accuracy: 0.4917\n",
            "Epoch 3/7\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.1349 - accuracy: 0.7333\n",
            "Epoch 4/7\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.7949 - accuracy: 0.8033\n",
            "Epoch 5/7\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5789 - accuracy: 0.8433\n",
            "Epoch 6/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4509 - accuracy: 0.8817\n",
            "Epoch 7/7\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3635 - accuracy: 0.9017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYoKkOIcjali"
      },
      "source": [
        "## Training and attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF107OjZjg7K"
      },
      "source": [
        "total_start = time.time()\n",
        "Test_loss = []\n",
        "Test_accuracy = []\n",
        "for i in range(Train_round):\n",
        "  start = time.time()\n",
        "  #Round robin\n",
        "  for j in range(10):\n",
        "    data_amount,Gradient = clients[j].training_step()\n",
        "    Gradient = np.array(Gradient) * data_amount/Total_data_amount\n",
        "    server.new_gradient(Gradient)\n",
        "\n",
        "  end = time.time()\n",
        "  print(\"Time for round:\",i,\"is \",end-start)\n",
        "  test_loss, Test_acc = server.model.evaluate(test_images,test_labels,verbose=0)\n",
        "  print(\"test_loss:\",test_loss,\"test_acc:\",Test_acc)\n",
        "  Test_loss.append(test_loss)\n",
        "  Test_accuracy.append(Test_acc)\n",
        "  plt.plot(Test_accuracy)\n",
        "  plt.show()\n",
        "print(\"total time:\",time.time()-total_start) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}